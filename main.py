# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sC5j6DD-tAJPkA-yQ7r4qWk2XSowO5_a
"""

import pandas as pd
import numpy as np
import arviz as az
import pymc as pm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import KFold
from pandas.plotting import scatter_matrix

# Print library versions
print("PyMC version:", pm.__version__)
print("ArviZ version:", az.__version__)

# Load and preprocess data
df = pd.read_csv('insurance.csv')
print("Shape of dataset:", df.shape)
print("\nColumns:\n", df.columns)
print("\nData types:\n", df.dtypes)
print("Missing values:\n", df.isnull().sum())

# Handle outliers by capping charges
df['charges'] = np.clip(df['charges'], a_min=0, a_max=50000)  # Cap at $50,000

# Visualize distribution of charges
plt.figure(figsize=(8, 4))
plt.hist(df['charges'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)
charges = df['charges']
density, bins = np.histogram(charges, bins=50, density=True)
centers = (bins[:-1] + bins[1:]) / 2
plt.plot(centers, density, color='red', linewidth=2, label='Density Estimate')
plt.title("Distribution of Insurance Charges (Capped)")
plt.xlabel("Charges")
plt.ylabel("Frequency")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Categorical breakdown
print(df['sex'].value_counts())
print(df['smoker'].value_counts())
print(df['region'].value_counts())

# Scatter matrix
df_color = df.copy()
df_color['smoker_color'] = df_color['smoker'].map({'yes': 'red', 'no': 'blue'})
numeric_cols = ['age', 'bmi', 'children', 'charges']
scatter_matrix(
    df_color[numeric_cols],
    figsize=(10, 10),
    alpha=0.8,
    diagonal='hist',
    color=df_color['smoker_color'],
    hist_kwds={'bins': 20, 'edgecolor': 'black'}
)
plt.suptitle("Scatter Matrix of Numeric Features by Smoking Status", y=1.02)
plt.figtext(0.88, 0.88, "Red = Smoker\nBlue = Non-Smoker", fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))
plt.tight_layout()
plt.show()

# Preprocessing with feature engineering
df_encoded = pd.get_dummies(df, columns=['sex', 'smoker', 'region'], drop_first=True)
df_encoded['age_smoker'] = df_encoded['age'] * df_encoded['smoker_yes']
df_encoded['bmi_squared'] = df_encoded['bmi'] ** 2
df_encoded['bmi_smoker'] = df_encoded['bmi'] * df_encoded['smoker_yes']
df_encoded['age_squared'] = df_encoded['age'] ** 2
df_encoded['children_smoker'] = df_encoded['children'] * df_encoded['smoker_yes']
df_encoded['bmi_age'] = df_encoded['bmi'] * df_encoded['age']
print(df_encoded.columns)

# Feature-target split
X = df_encoded.drop(['charges', 'region_northwest', 'region_southeast', 'region_southwest'], axis=1)
y = df_encoded['charges']
print("X shape:", X.shape, "y shape:", y.shape)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
print("X_scaled_df shape:", X_scaled_df.shape, "y shape:", y.shape)

# Log-transform target
y_array = np.log(y.values)  # Log-transform charges
X_array = X_scaled_df.values
n_features = X_array.shape[1]
print("X_array shape:", X_array.shape, "y_array shape:", y_array.shape)

# Verify input data
if X_array.shape[0] != 1338 or y_array.shape[0] != 1338:
    raise ValueError(f"Expected 1338 observations, got X_array: {X_array.shape[0]}, y_array: {y_array.shape[0]}")

# Feature importance from Linear Regression
lr_temp = LinearRegression()
lr_temp.fit(X_scaled_df, y_array)
feature_importance = pd.Series(lr_temp.coef_, index=X_scaled_df.columns).abs()
print("\nFeature Importance (Linear Regression Coefficients):\n", feature_importance.sort_values(ascending=False))

# Get initial values from Linear Regression
init_intercept = np.mean(y_array)
init_coeffs = lr_temp.coef_ / 100
init_vals = {
    "intercept": init_intercept,
    "coefficients": init_coeffs,
    "sigma_log__": np.log(0.5)  # Initial sigma = 0.5
}

# Define Bayesian model with Normal likelihood
with pm.Model() as model:
    intercept = pm.Normal("intercept", mu=np.mean(y_array), sigma=1)
    coefficients = pm.Laplace("coefficients", mu=0, b=0.5, shape=n_features)
    sigma = pm.HalfCauchy("sigma", beta=5)

    # Compute mu
    mu_linear = intercept + pm.math.dot(X_array, coefficients)
    pm.Deterministic("mu_pred", mu_linear)

    y_obs = pm.Normal("y_obs", mu=mu_linear, sigma=sigma, observed=y_array)

    try:
        trace = pm.sample(1500, tune=1500, chains=3, target_accept=0.95, initvals=init_vals, return_inferencedata=True, random_seed=42)
    except Exception as e:
        print("Sampling failed:", e)
        raise

# Posterior predictive sampling
with model:
    posterior_pred = pm.sample_posterior_predictive(
        trace,
        var_names=["y_obs"],
        random_seed=42
    )

# Check shapes
print("y_obs shape:", posterior_pred.posterior_predictive["y_obs"].shape)
pred_samples = posterior_pred.posterior_predictive["y_obs"].stack(sample=("chain", "draw")).values
print("pred_samples shape:", pred_samples.shape)

# Fix shape if necessary
if pred_samples.shape[0] == 1338 and pred_samples.shape[1] == 4500:
    print("Transposing pred_samples to correct shape...")
    pred_samples = pred_samples.T  # Transpose to (4500, 1338)
    print("Corrected pred_samples shape:", pred_samples.shape)

# Ensure pred_samples has shape (n_samples, 1338)
if pred_samples.shape[-1] != 1338:
    raise ValueError(f"Expected pred_samples to have 1338 data points, got {pred_samples.shape[-1]}")

# Compute prediction mean and HDI (log scale)
pred_means = pred_samples.mean(axis=0)  # Shape: (1338,)
pred_hdi = az.hdi(pred_samples, hdi_prob=0.94)  # Shape: (1338, 2)

# Exponentiate predictions for evaluation
pred_means_exp = np.exp(pred_means)
pred_hdi_exp = np.exp(pred_hdi)
y_true = y.values  # Original scale (dollars)

# Verify shapes
print("pred_means_exp shape:", pred_means_exp.shape)
print("pred_hdi_exp shape:", pred_hdi_exp.shape)
print("y_true shape:", y_true.shape)

# Evaluate Bayesian predictions
if len(pred_means_exp) != len(y_true):
    raise ValueError(f"Shape mismatch: pred_means_exp ({len(pred_means_exp)}) vs y_true ({len(y_true)})")

# Cross-validation for Bayesian model
kf = KFold(n_splits=5, shuffle=True, random_state=42)
rmse_bayes_cv, mae_bayes_cv, r2_bayes_cv = [], [], []
for train_idx, test_idx in kf.split(X_array):
    X_train, X_test = X_array[train_idx], X_array[test_idx]
    y_train, y_test = y_array[train_idx], y_array[test_idx]
    y_test_exp = np.exp(y_test)  # Exponentiate for evaluation

    print(f"CV Fold - X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

    # Update init_vals for cross-validation
    lr_cv = LinearRegression()
    lr_cv.fit(X_train, y_train)
    init_vals_cv = {
        "intercept": np.mean(y_train),
        "coefficients": lr_cv.coef_ / 100,
        "sigma_log__": np.log(0.5)
    }

    with pm.Model() as cv_model:
        intercept = pm.Normal("intercept", mu=np.mean(y_train), sigma=1)
        coefficients = pm.Laplace("coefficients", mu=0, b=0.5, shape=n_features)
        sigma = pm.HalfCauchy("sigma", beta=5)

        # Define mu for training data
        mu_linear = intercept + pm.math.dot(X_train, coefficients)
        y_obs = pm.Normal("y_obs", mu=mu_linear, sigma=sigma, observed=y_train)
        trace_cv = pm.sample(1500, tune=1500, chains=3, target_accept=0.95, initvals=init_vals_cv, return_inferencedata=True, random_seed=42)

    # Predict for test set using posterior predictive sampling
    with pm.Model() as pred_model:
        intercept = pm.Normal("intercept", mu=np.mean(y_train), sigma=1)
        coefficients = pm.Laplace("coefficients", mu=0, b=0.5, shape=n_features)
        sigma = pm.HalfCauchy("sigma", beta=5)

        # Compute mu for test data
        mu_linear = intercept + pm.math.dot(X_test, coefficients)
        y_obs = pm.Normal("y_obs", mu=mu_linear, sigma=sigma)

        posterior_pred_cv = pm.sample_posterior_predictive(trace_cv, var_names=["y_obs"], random_seed=42)

    pred_means_cv = posterior_pred_cv.posterior_predictive["y_obs"].mean(dim=["chain", "draw"]).values
    pred_means_cv_exp = np.exp(pred_means_cv)  # Exponentiate for evaluation
    print(f"CV Fold - pred_means_cv_exp shape: {pred_means_cv_exp.shape}, y_test_exp shape: {y_test_exp.shape}")

    if len(pred_means_cv_exp) != len(y_test_exp):
        raise ValueError(f"Shape mismatch in CV: pred_means_cv_exp ({len(pred_means_cv_exp)}) vs y_test_exp ({len(y_test_exp)})")
    rmse_bayes_cv.append(np.sqrt(mean_squared_error(y_test_exp, pred_means_cv_exp)))
    mae_bayes_cv.append(mean_absolute_error(y_test_exp, pred_means_cv_exp))
    r2_bayes_cv.append(r2_score(y_test_exp, pred_means_cv_exp))

rmse_bayes = np.mean(rmse_bayes_cv)
mae_bayes = np.mean(mae_bayes_cv)
r2_bayes = np.mean(r2_bayes_cv)

# Cross-validation for Linear Regression
rmse_lr_cv, mae_lr_cv, r2_lr_cv = [], [], []
for train_idx, test_idx in kf.split(X_scaled_df):
    X_train, X_test = X_scaled_df.iloc[train_idx], X_scaled_df.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    y_pred_lr = lr.predict(X_test)
    rmse_lr_cv.append(np.sqrt(mean_squared_error(y_test, y_pred_lr)))
    mae_lr_cv.append(mean_absolute_error(y_test, y_pred_lr))
    r2_lr_cv.append(r2_score(y_test, y_pred_lr))

rmse_lr = np.mean(rmse_lr_cv)
mae_lr = np.mean(mae_lr_cv)
r2_lr = np.mean(r2_lr_cv)

# Plot predictions
plt.figure(figsize=(10, 5))
plt.plot(y_true, label="Actual Charges", alpha=0.7)
plt.plot(pred_means_exp, label="Bayesian Predicted Mean", color='red')
plt.fill_between(np.arange(len(y_true)), pred_hdi_exp[:, 0], pred_hdi_exp[:, 1],
                 color='red', alpha=0.3, label="94% Credible Interval")
plt.title("Bayesian Predictions vs Actual Charges")
plt.xlabel("Observation Index")
plt.ylabel("Insurance Charges (Dollars)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Residual plot
plt.figure(figsize=(8, 5))
plt.scatter(pred_means_exp, y_true - pred_means_exp, alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicted Charges (Dollars)")
plt.ylabel("Residuals (Actual - Predicted)")
plt.title("Residual Plot for Bayesian Model")
plt.grid(True)
plt.show()

# Predicted vs Actual plot
plt.figure(figsize=(8, 5))
plt.scatter(y_true, pred_means_exp, alpha=0.5)
plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
plt.xlabel("Actual Charges (Dollars)")
plt.ylabel("Predicted Charges (Dollars)")
plt.title("Predicted vs Actual Charges")
plt.grid(True)
plt.tight_layout()
plt.show()

# HDI coverage plot
plt.figure(figsize=(8, 5))
plt.scatter(range(len(y_true)), y_true, alpha=0.5, label="Actual Charges")
plt.fill_between(range(len(y_true)), pred_hdi_exp[:, 0], pred_hdi_exp[:, 1], color='red', alpha=0.3, label="94% HDI")
plt.title("Actual Charges with 94% HDI")
plt.xlabel("Observation Index")
plt.ylabel("Insurance Charges (Dollars)")
plt.legend()
plt.grid(True)
plt.show()

# HDI width plot
hdi_width = pred_hdi_exp[:, 1] - pred_hdi_exp[:, 0]
plt.figure(figsize=(8, 5))
plt.hist(hdi_width, bins=50, color='steelblue', edgecolor='black', alpha=0.7)
plt.title("Distribution of 94% HDI Widths")
plt.xlabel("HDI Width (Dollars)")
plt.ylabel("Frequency")
plt.grid(True)
plt.tight_layout()
plt.show()

# Posterior predictive samples plot
plt.figure(figsize=(8, 5))
plt.hist(np.exp(pred_samples[::10, :].flatten()), bins=50, color='green', alpha=0.7, density=True, label='Posterior Predictive')
plt.hist(y_true, bins=50, color='blue', alpha=0.5, density=True, label='Actual Charges')
plt.title("Posterior Predictive vs Actual Charges Distribution")
plt.xlabel("Charges (Dollars)")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Bayesian feature importance plot
bayes_feature_importance = pd.Series(np.abs(trace.posterior["coefficients"].mean(dim=["chain", "draw"]).values), index=X_scaled_df.columns)
plt.figure(figsize=(10, 6))
bayes_feature_importance.sort_values().plot(kind='barh', color='teal')
plt.title("Bayesian Feature Importance (Posterior Mean Coefficients)")
plt.xlabel("Absolute Coefficient Value")
plt.tight_layout()
plt.show()

# Trace and summary
az.plot_trace(trace, var_names=["intercept", "sigma", "coefficients"])
plt.tight_layout()
plt.show()
print(az.summary(trace, var_names=["intercept", "sigma", "coefficients"]))

# HDI coverage
within_hdi = (y_true >= pred_hdi_exp[:, 0]) & (y_true <= pred_hdi_exp[:, 1])
coverage = within_hdi.mean()
print(f"\n✅ 94% HDI Coverage: {coverage:.2%}")

# Final metrics
print("\n📊 Model Performance Comparison (Cross-Validated):")
print(f"Linear Regression  → RMSE: {rmse_lr:.2f}, MAE: {mae_lr:.2f}, R²: {r2_lr:.4f}")
print(f"Bayesian Regression → RMSE: {rmse_bayes:.2f}, MAE: {mae_bayes:.2f}, R²: {r2_bayes:.4f}")